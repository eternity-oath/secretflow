# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Ant Group Co., Ltd.
# This file is distributed under the same license as the SecretFlow package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: SecretFlow \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-04-27 13:22+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"

#: ../../user_guide/mpc_ml/linear_model.rst:2
msgid "Linear Models"
msgstr "线性回归模型"

#: ../../user_guide/mpc_ml/linear_model.rst:4
msgid ""
"Linear model is a kind of statistical model with simple form and very "
"widely used. Under the protection of a multi-party secure computing "
"protocol, SecretFlow implements provably secure linear regression and "
"binary classification regression through `minibatch Stochastic Gradient "
"Descent (SGD) method <https://d2l.ai/chapter_optimization/minibatch-"
"sgd.html#minibatches>`_ for vertically partitioned dataset setting."
msgstr ""
"线性回归模型是形式简单且使用非常广泛的一类统计模型， 隐语在多方安全计算协议的保护下， 通过 `minibatch梯度下降方法 "
"<https://d2l.ai/chapter_optimization/minibatch-sgd.html#minibatches>`_ 实现了可证安全的线性回归和二分类回归。 "

#: ../../user_guide/mpc_ml/linear_model.rst:11
msgid ""
"The matrix formula for minibatch Stochastic Gradient Descent (SGD) is as "
"follows:"
msgstr "minibatch梯度下降的矩阵化公式如下："

#: ../../user_guide/mpc_ml/linear_model.rst:13
msgid "Normal:"
msgstr "常规："

#: ../../user_guide/mpc_ml/linear_model.rst:15
msgid ""
":math:`{\\theta^{t+1}} \\leftarrow {\\theta^t} - \\frac{\\alpha}{m}  "
"{X}^T ({X}{\\theta^t} - {y})`"
msgstr ""
":math:`{\\theta^{t+1}} \\leftarrow {\\theta^t} - \\frac{\\alpha}{m}  "
"{X}^T ({X}{\\theta^t} - {y})`"

#: ../../user_guide/mpc_ml/linear_model.rst:17
msgid "L2 penalty:"
msgstr "L2正则："

#: ../../user_guide/mpc_ml/linear_model.rst:19
msgid ""
":math:`{\\theta^{t+1}} \\leftarrow {\\theta^t} - \\frac{\\alpha}{m}  "
"({X}^T ({X}{\\theta^t} - {y}) + \\lambda {w^t})` where :math:`w^t_0 = 0, "
"w^t_j = \\theta^t_j` :math:`j = 1, \\cdots, n`"
msgstr ""
":math:`{\\theta^{t+1}} \\leftarrow {\\theta^t} - \\frac{\\alpha}{m}  "
"({X}^T ({X}{\\theta^t} - {y}) + \\lambda {w^t})` 其中 :math:`w^t_0 = 0, "
"w^t_j = \\theta^t_j` :math:`j = 1, \\cdots, n`"

#: ../../user_guide/mpc_ml/linear_model.rst:24
msgid "SecretFlow provides two provably security implementations of SGD:"
msgstr "隐语提供了 SS-SGD/HESS-SGD 两种可证安全实现："

#: ../../user_guide/mpc_ml/linear_model.rst:26
msgid ""
"SS-SGD: SS-SGD is short for secret sharing SGD training, uses Secret "
"Sharing to calculate the gradient."
msgstr "SS-SGD: SS-SGD 是 secret sharing SGD training 的缩写，使用秘密分享协议计算下降梯度。"

#: ../../user_guide/mpc_ml/linear_model.rst:28
msgid ""
"HESS-SGD: HESS-SGD is short for HE & secret sharing SGD training, uses "
"homomorphic encryption to calculate the gradient."
msgstr "HESS-SGD: HESS-SGD 是 HE & secret sharing SGD training 的缩写，使用同态加密计算下降梯度。"

#: ../../user_guide/mpc_ml/linear_model.rst:30
msgid ""
"Secret Sharing is sensitive to bandwidth and latency, and the homomorphic"
" encryption scheme consumes more CPU power."
msgstr "Secret Sharing对带宽和延迟比较敏感，而同态加密方案会消耗更多的cpu算力。"

#: ../../user_guide/mpc_ml/linear_model.rst:32
msgid ""
"Secret Sharing can complete the modeling faster with LAN or 10 Gigabit "
"network, and with limited bandwidth and high latency network environment "
"can use HE to improve the modeling speed."
msgstr "局域网/万兆环境下SS能更快的完成建模，带宽受限且延迟较高的网络环境可以用HE提高建模速度。"

#: ../../user_guide/mpc_ml/linear_model.rst:35
msgid ""
"The two implementations have basically the same logic and algorithm "
"security settings other than gradient calculation. According to the "
"different CPU/bandwidth of the running environment, you can choose the "
"appropriate implementation to use."
msgstr "两种实现方式在求梯度之外的运算逻辑和算法的安全设定基本一致，根据用户环境的cpu/带宽不同，选择合适的实现使用即可。"

#: ../../user_guide/mpc_ml/linear_model.rst:40
msgid "SS-SGD"
msgstr "SS-SGD"

#: ../../user_guide/mpc_ml/linear_model.rst:42
msgid ""
"The SS-SGD module "
":py:meth:`~secretflow.ml.linear.ss_sgd.model.SSRegression` provides both "
"linear and logistic regression linear models for vertical split dataset "
"setting by using secret sharing with mini batch SGD training solver."
msgstr ""
":py:meth:`~secretflow.ml.linear.ss_sgd.model.SSRegression` "
"使用秘密分享协议实现了针对垂直划分数据集的线性回归和二分类回归训练，求解方式为批量随机梯度下降。"

#: ../../user_guide/mpc_ml/linear_model.rst:47
msgid ""
"`Linear regression <https://d2l.ai/chapter_linear-regression/linear-"
"regression.html#basics>`_ fits a linear model with coefficients w = (w1, "
"..., wp) to minimize the residual sum of squares between the observed "
"targets in the dataset, and the targets predicted by the linear "
"approximation."
msgstr ""
"`线性回归 <https://d2l.ai/chapter_linear-regression/linear-"
"regression.html#basics>`_ 拟合系数为 w = (w1, "
"..., wp) 的线性模型，目标为最小化数据集中标签值与线性近似的预测值之间的残差平方和。"

#: ../../user_guide/mpc_ml/linear_model.rst:52
msgid ""
"`Logistic regression <https://d2l.ai/chapter_linear-classification"
"/softmax-regression.html#linear-model>`_ , despite its name, is a linear "
"model for classification rather than regression. logistic regression is "
"also known in the literature as logit regression, maximum-entropy "
"classification (MaxEnt) or the log-linear classifier. the probabilities "
"describing the possible outcomes of a single trial are modeled using a "
"logistic function. This method can fit binary regularization with "
"optional L2 regularization."
msgstr ""
"`逻辑回归 <https://d2l.ai/chapter_linear-classification"
"/softmax-regression.html#linear-model>`_ "
"是以分类为目标的线性模型。逻辑回归在文献中也称为 logit 回归、最大熵分类 (MaxEnt) "
"或对数线性分类器。通过逻辑函数将线性预测结果转换为样本的概率结果。该方法同时提供可选的L2 正则化选项来防止过拟合。"

#: ../../user_guide/mpc_ml/linear_model.rst:61
#: ../../user_guide/mpc_ml/linear_model.rst:220
msgid "Example"
msgstr "用例"

#: ../../user_guide/mpc_ml/linear_model.rst:63
#: ../../user_guide/mpc_ml/linear_model.rst:222
msgid ""
"A local cluster(Standalone Mode) needs to be initialized as the running "
"environment for this example. See `Deployment "
"<https://www.secretflow.org.cn/docs/secretflow/latest/en-US/getting_started/deployment.html>`_ and refer to the 'Cluster "
"Mode'."
msgstr ""
"在本示例中使用单节点模式做示范。集群模式的部署方式： `Deployment "
"<https://www.secretflow.org.cn/docs/secretflow/latest/zh-Hans/getting_started/deployment.html>`_ "

#: ../../user_guide/mpc_ml/linear_model.rst:66
msgid ""
"For more detail about parameter settings, see API "
":py:meth:`~secretflow.ml.linear.ss_sgd.model.SSRegression`"
msgstr "API详情：:py:meth:`~secretflow.ml.linear.ss_sgd.model.SSRegression`"

#: ../../user_guide/mpc_ml/linear_model.rst:174
msgid "algorithm details"
msgstr "算法实现"

#: ../../user_guide/mpc_ml/linear_model.rst:175
msgid "more detail for logistic regression:"
msgstr "逻辑回归详细流程："

#: ../../user_guide/mpc_ml/linear_model.rst:177
msgid "Taking binary regression as an example, the main process is as follows:"
msgstr "以二分类为例，主要流程如下"

#: ../../user_guide/mpc_ml/linear_model.rst:179
msgid "Step 1: Initialize the dataset"
msgstr "Step 1: 初始化数据集"

#: ../../user_guide/mpc_ml/linear_model.rst:181
msgid ""
"The data provider infeed their dataset into secret sharing and vertically"
" concatenates them as X."
msgstr "数据提供方分别将数据集Secret Share进入密态，并在密态下concatenate为X。"

#: ../../user_guide/mpc_ml/linear_model.rst:182
msgid "The data provide holds Y infeed it into Secret Sharing."
msgstr "Y数据持有方将Y Secret Share进入密态."

#: ../../user_guide/mpc_ml/linear_model.rst:183
msgid ""
"Initialize weights w to the initial value set in parameter under Secret "
"Sharing."
msgstr "在Secret Sharing下初始化w为设定的初始值。"

#: ../../user_guide/mpc_ml/linear_model.rst:184
#: ../../user_guide/mpc_ml/linear_model.rst:344
msgid ""
"X.rows must be greater than X.cols, otherwise: 1. model will not "
"converge; 2. Y may leak."
msgstr "数据集要求 X.rows > X.cols：1、样本数过少模型不收敛；2、Y可能会泄漏。"

#: ../../user_guide/mpc_ml/linear_model.rst:186
#: ../../user_guide/mpc_ml/linear_model.rst:350
msgid ""
"Step 2: Using mini-batch gradient descent, repeat the following steps "
"until the target number of iterations is reached"
msgstr "Step 2：采用mini-batch梯度下降，重复执行如下步骤，直至到达目标迭代次数"

#: ../../user_guide/mpc_ml/linear_model.rst:188
msgid ""
"Step 2.1: Calculate the predicted value: pred = sigmoid(batch_x * w). The"
" sigmoid can be approximated using Taylor expansion, piecewise function, "
"inverse square sigmoid function, etc."
msgstr "Step 2.1：计算预测值 pred = sigmoid(batch_x * w)。sigmoid可使用泰勒展开、分段函数、根号逆S形函数等近似。"

#: ../../user_guide/mpc_ml/linear_model.rst:190
msgid "Step 2.2: Calculate: err = pred - y"
msgstr "Step 2.2：计算err = pred - y"

#: ../../user_guide/mpc_ml/linear_model.rst:191
msgid "Step 2.3: Calculate the gradient: grad = batch_x.transpose() * err"
msgstr "Step 2.3：计算梯度 grad = batch_x.transpose() * err"

#: ../../user_guide/mpc_ml/linear_model.rst:192
msgid ""
"Step 2.4: If using L2 penalty, update gradient: grad = grad + w' * "
"l2_norm, where the intercept term of w' is 0"
msgstr "Step 2.4：如果使用 L2 penalty，更新梯度 grad = grad + w' * l2_norm，其中w'的截距项为0"

#: ../../user_guide/mpc_ml/linear_model.rst:193
msgid "Step 2.5: update weights: w = w - (grad * learning_rate / batch_size)"
msgstr "Step 2.5：迭代权重 w = w - (grad * learning_rate / batch_size)"

#: ../../user_guide/mpc_ml/linear_model.rst:195
msgid ""
"Step 3: Output - At this time, weights w is in the secret sharing. You "
"can output reveal (w) as plaintext or directly save the secret sharing as"
" needed."
msgstr "Step 3：输出模型，此时w处在Secret Sharing状态。根据需要可以将reveal (w)为明文输出或直接保存密态分片。"

#: ../../user_guide/mpc_ml/linear_model.rst:199
#: ../../user_guide/mpc_ml/linear_model.rst:373
msgid "Security Analysis"
msgstr "安全性分析"

#: ../../user_guide/mpc_ml/linear_model.rst:201
msgid ""
"The X/Y/W participating in the calculation are kept in the Secret Sharing"
" from the beginning. And there is no reveal operation in the calculation "
"process, so it is impossible to infer the information of the plaintext "
"data through the interactive data in the calculation."
msgstr ""
"参与计算的X/Y/W从一开始就保持在Secret Sharing状态进行联合运算，并且在运算过程中不存在reveal操作, "
"所以通过计算中交互的数据是无法推断出明文数据的信息。"

#: ../../user_guide/mpc_ml/linear_model.rst:206
msgid "HESS-SGD"
msgstr "HESS-SGD"

#: ../../user_guide/mpc_ml/linear_model.rst:208
msgid ""
"The HESS-SGD module "
":py:meth:`~secretflow.ml.linear.hess_sgd.model.HESSLogisticRegression` "
"implements provably secure linear regression using homomorphic encryption"
" and Secret Sharing."
msgstr ""
"HESS-SGD模块 "
":py:meth:`~secretflow.ml.linear.hess_sgd.model.HESSLogisticRegression` "
"混合使用同态加密和Secret Sharing实现了可证安全的线性回归."

#: ../../user_guide/mpc_ml/linear_model.rst:211
msgid ""
"The biggest difference from SS-SGD is that the gradient calculation which"
" has the largest communication cost in SS-SGD is replaced by locally "
"homomorphic calculation implementation. Due to the asymmetric nature of "
"homomorphic encryption, currently HESS-SGD only supports 2PC. The "
"algorithm implementation reference is `<When Homomorphic Encryption "
"Marries Secret Sharing: Secure Large-Scale Sparse Logistic Regression and"
" Applications in Risk Control> <https://arxiv.org/pdf/2008.08753.pdf>`_, "
"and some engineering optimizations are carried out."
msgstr ""
"和SS-SGD最大的区别在于：将SS-SGD中通讯开销最大的梯度计算替换为纯本地的同态计算实现。由于同态加密的非对称特性，目前HESS-"
"SGD只支持2方计算。算法实现参考 `<When Homomorphic Encryption Marries Secret Sharing: "
"Secure Large-Scale Sparse Logistic Regression and Applications in Risk "
"Control> <https://arxiv.org/pdf/2008.08753.pdf>`_，并进行一些工程优化改造。"

#: ../../user_guide/mpc_ml/linear_model.rst:225
msgid ""
"For more details about API, see "
":py:meth:`~secretflow.ml.linear.hess_sgd.model.HESSLogisticRegression`"
msgstr ""
"API详情 "
":py:meth:`~secretflow.ml.linear.hess_sgd.model.HESSLogisticRegression`"

#: ../../user_guide/mpc_ml/linear_model.rst:338
msgid "Algorithm Details"
msgstr "算法详情"

#: ../../user_guide/mpc_ml/linear_model.rst:340
msgid "the main process is as follows:"
msgstr "主要流程如下："

#: ../../user_guide/mpc_ml/linear_model.rst:342
msgid "Step 1: Initialize"
msgstr "Step 1: 初始化数据集"

#: ../../user_guide/mpc_ml/linear_model.rst:345
msgid "Y must be held by Bob"
msgstr "必须由bob持有Y"

#: ../../user_guide/mpc_ml/linear_model.rst:346
msgid ""
"Initialize w1 / w2, which are the weights corresponding to the features "
"held by Alice / Bob."
msgstr "初始化w1 / w2，分别为 Alice / Bob 持有特征对应的权重。"

#: ../../user_guide/mpc_ml/linear_model.rst:347
msgid ""
"Use Bob's pk to encrypt w1 -> hw1, and the ciphertext hw1 is stored in "
"Alice. Use Alice's pk to encrypt w2 -> hw2, and the ciphertext hw2 is "
"stored in Bob."
msgstr "使用Bob的pk加密w1 -> hw1，密文保存在Alice。使用Alice的pk加密w2 -> hw2，密文保存在Bob。"

#: ../../user_guide/mpc_ml/linear_model.rst:352
msgid "Alice / Bob read x1 / x2, y for current batch as plaintext."
msgstr "Alice / Bob 分别读取当前batch的 x1 / x2，y"

#: ../../user_guide/mpc_ml/linear_model.rst:353
msgid ""
"Use Bob's pk to encrypt x1 -> hx1, and the ciphertext hx1 is stored in "
"Alice. Use Alice's pk to encrypt x2 -> hx2, and the ciphertext hx2 is "
"stored in Bob."
msgstr "使用Bob的pk加密x1 -> hx1，密文保存在Alice。使用Alice的pk加密x2 -> hx2，密文保存在Bob。"

#: ../../user_guide/mpc_ml/linear_model.rst:355
msgid "Bob infeed y into Secret Sharing <y>"
msgstr "Bob通过Secret Sharing共享 <y>"

#: ../../user_guide/mpc_ml/linear_model.rst:356
msgid ""
"Alice locally computes partial predictions hp1 = hx1 * hw1 in homomorphic"
" encryption, Bob locally computes partial predictions hp2 = hx2 * hw2 in "
"homomorphic encryption."
msgstr "Alice 本地计算部分预测值 hp1 = hx1 * hw1，Bob本地计算部分预测值 hp2 = hx2 * hw2。"

#: ../../user_guide/mpc_ml/linear_model.rst:358
msgid ""
"Convert homomorphic encrypted predictions to secret sharing by H2S "
"operations: H2S(hp1) -> <p1> , H2S(hp2) -> <p2>"
msgstr "使用H2S操作将同态加密的预测值进行转换为secret sharing：H2S(hp1) -> <p1>，H2S(hp2) -> <p2>"

#: ../../user_guide/mpc_ml/linear_model.rst:359
msgid ""
"Calculate <error>=Sigmoid(<p1> + <p2>) - <y> in secret sharing, the "
"Sigmoid function approximates using y = 0.5 + 0.125 * x"
msgstr "计算 <error>=Sigmoid(<p1> + <p2>) - <y>，Sigmoid 函数使用 y = 0.5 + 0.125 * x 近似"

#: ../../user_guide/mpc_ml/linear_model.rst:361
msgid ""
"Use Bob's pk to reduce secret sharing to homomorphic encrypted "
"S2H(<error>) -> he1, and the ciphertext he1 is stored in Alice. Use "
"Alice's pk to reduce secret sharing to homomorphic encrypted S2H(<error>)"
" -> he2, and the ciphertext he2 is stored in Bob."
msgstr ""
"使用Bob的pk进行S2H(<error>) -> he1，密文保存在Alice。使用Alice的pk进行S2H(<error>) -> "
"he2，密文保存在Bob。"

#: ../../user_guide/mpc_ml/linear_model.rst:363
msgid ""
"Alice locally computes hw1 = hw1 - he1 * hx1 * learning_rate in "
"homomorphic encryption, Bob locally computes hw2 = hw2 - he2 * hx2 * "
"learning_rate in homomorphic encryption."
msgstr ""
"Alice本地计算hw1 = hw1 - he1 * hx1 * learning_rate，Bob本地计算hw2 = hw2 - he2 * "
"hx2 * learning_rate。"

#: ../../user_guide/mpc_ml/linear_model.rst:366
msgid "Step 3: Output"
msgstr "Step 3：输出模型"

#: ../../user_guide/mpc_ml/linear_model.rst:368
msgid ""
"Convert hw1, hw2 to secret sharing using H2S operation: H2S(hw1) -> <w1> "
", H2S(hw2) -> <w2>"
msgstr "使用H2S操作将hw1，hw2转换为secret sharing：H2S(hw1) -> <w1> ，H2S(hw2) -> <w2>"

#: ../../user_guide/mpc_ml/linear_model.rst:369
msgid "<w> = concatenate(<w1>, <w2>)"
msgstr "<w> = concatenate(<w1>，<w2>)"

#: ../../user_guide/mpc_ml/linear_model.rst:375
msgid ""
"First, analyze the data interaction in the calculation process to see if "
"there is plaintext information leakage. There are two types of data "
"interaction in the calculation process:"
msgstr "首先分析在计算过程中的数据交互，是否存在明文信息泄漏。在计算的过程存在2类数据交互："

#: ../../user_guide/mpc_ml/linear_model.rst:378
msgid ""
"-> Marked HE encryption and decryption process and H2S/S2H encryption "
"state conversion:"
msgstr "-> 标记的HE加解密过程和H2S/S2H密态转换："

#: ../../user_guide/mpc_ml/linear_model.rst:380
msgid ""
"The security of the HE encryption and decryption process completely "
"depends on the algorithm itself."
msgstr "HE加解密过程安全性完全依赖算法本身。"

#: ../../user_guide/mpc_ml/linear_model.rst:381
msgid ""
"When H2S creates a secret sharing, it will first mark the random number "
"in the ciphertext and then decrypt it, without leaking the plaintext "
"information."
msgstr "H2S创建分片时，会先在密文mark随机数再解密，没有泄漏明文信息。"

#: ../../user_guide/mpc_ml/linear_model.rst:383
msgid ""
"S2H will first encrypt one party's shard, and then reduce other shards on"
" the ciphertext, without leaking plaintext information."
msgstr "S2H会先加密一方的分片，然后在密文上reduce其他分片，没有泄漏明文信息。"

#: ../../user_guide/mpc_ml/linear_model.rst:386
msgid ""
"The interaction in the secret sharing and the computing. The security of "
"these processes depends on the mpc protocol used, Taking the default ABY3"
" protocol as an example, in the case of no collusion between SPU nodes, "
"it can be guaranteed that no plaintext information can be returned by "
"analyzing the data exchanged between nodes."
msgstr ""
"Secret "
"Sharing密态下的分享和联合计算过程中存在的交互。这些过程的安全性依赖于使用的mpc协议，以默认的ABY3协议为例，在SPU节点无共谋的情况下，可以保证通过分析节点间交互的数据，无法返推出任何明文信息。"

#: ../../user_guide/mpc_ml/linear_model.rst:390
msgid ""
"The final output result <w> is stored in the Secret Sharing state, and "
"any w-related information cannot be reversed before reveal <w>."
msgstr "最终输出的结果保存在Secret Sharing密态下，在reveal前无法反推出任何w相关信息。"


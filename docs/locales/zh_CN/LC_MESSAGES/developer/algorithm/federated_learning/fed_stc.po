# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Ant Group Co., Ltd.
# This file is distributed under the same license as the SecretFlow package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: SecretFlow \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-07-03 15:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../developer/algorithm/federated_learning/fed_stc.md:1
msgid "Strategy: FedSTC"
msgstr "联邦策略：FedSTC"

#: ../../developer/algorithm/federated_learning/fed_stc.md:3
msgid "Overview"
msgstr "概览"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Sparse method"
msgstr "Sparse method"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Quant method"
msgstr "Quant method"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Residual"
msgstr "Residual"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Encoding"
msgstr "Encoding"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Upstream"
msgstr "Upstream"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Downstream"
msgstr "Downstream"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "FedSTC"
msgstr "FedSTC"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "topk"
msgstr "topk"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "binarization"
msgstr "binarization"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Yes"
msgstr "Yes"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Golomb"
msgstr "Golomb"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Handle Non-IID"
msgstr "Handle Non-IID"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Handle Dropping/Skipping"
msgstr "Handle Dropping/Skipping"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Generality"
msgstr "Generality"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Fine //TODO"
msgstr "Fine //TODO"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Caching and synchronizing"
msgstr "Caching and synchronizing"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "General"
msgstr "General"

#: ../../developer/algorithm/federated_learning/fed_stc.md:11
msgid ""
"The main motivation of FedSTC is to compress the communication between "
"client and server. The main contributions are as follows:"
msgstr "FedSTC的主要motivation是为client和server之间的通讯做压缩，主要的贡献如下"

#: ../../developer/algorithm/federated_learning/fed_stc.md:13
msgid ""
"Compared with the previous sparse work on upstream (client 2 server), "
"FedSTC also sparses on downstream (server 2 client);"
msgstr ""
"相比之前仅在upstream（client 2 server）上做稀疏化的工作，FedSTC在downstream（server 2 "
"client）上也做了稀疏化"

#: ../../developer/algorithm/federated_learning/fed_stc.md:14
msgid ""
"When only some clients participate in each round, a Weight Update Caching"
" mechanism is provided on the server side. Each client must synchronize "
"the latest model before participating in the next round of training, or "
"lag behind global weights. updates; (I understand such motivation is that"
" if only part of the updates are updated, the content to be transmitted "
"can be sparse);"
msgstr ""
"在每一轮只有部分client参与的情况下，在server侧提供了Weight Update "
"Caching的机制，每个client在参加下一轮训练之前必须同步最新的模型，或者是和global "
"weights相比落后的updates；（我理解这样的motivation是如果只更新部分updates，可以让要传输的内容是稀疏的）"

#: ../../developer/algorithm/federated_learning/fed_stc.md:15
msgid ""
"Quantization is added while sparse. The quantization method is "
"Binarization. Only 3 numbers will appear in the final matrix, "
"$\\{-\\mu,0,\\mu\\}$;"
msgstr "在做稀疏化的同时加上了量化，量化的方法是Binarization，最终的矩阵中只会出现3个数字，$\\{-\\mu,0,\\mu\\}$;"

#: ../../developer/algorithm/federated_learning/fed_stc.md:16
msgid "Lossless Golomb Encoding is used on the sparse + quantized matrix;"
msgstr "在稀疏+量化后的矩阵上使用了无损的Golomb Encoding"

#: ../../developer/algorithm/federated_learning/fed_stc.md:18
msgid "Design"
msgstr "设计"

#: ../../developer/algorithm/federated_learning/fed_stc.md:20
msgid "Sparsity（topk）"
msgstr "Sparsity（topk）"

#: ../../developer/algorithm/federated_learning/fed_stc.md:22
msgid ""
"Only upstream sparse: ![math1](resources/fedstc_math_1.jpg) Add "
"downstream： ![math2](resources/fedstc_math_2.jpg) A is the Residual "
"status on the server side of the previous round;"
msgstr ""
"仅有upstream sparse的情况: ![math1](resources/fedstc_math_1.jpg)加上downstream： "
"![math2](resources/fedstc_math_2.jpg) A是上一轮server侧的Residual，状态;"

#: ../../developer/algorithm/federated_learning/fed_stc.md:22
msgid "math1"
msgstr "math1"

#: ../../developer/algorithm/federated_learning/fed_stc.md:22
msgid "math2"
msgstr "math2"

#: ../../developer/algorithm/federated_learning/fed_stc.md:28
msgid "Caching"
msgstr "Caching"

#: ../../developer/algorithm/federated_learning/fed_stc.md:30
msgid "The server keeps the most recent historical updates:"
msgstr ""

#: ../../developer/algorithm/federated_learning/fed_stc.md:32
msgid "![math3](resources/fedstc_math_3.jpg)"
msgstr ""

#: ../../developer/algorithm/federated_learning/fed_stc.md:32
msgid "math3"
msgstr "math3"

#: ../../developer/algorithm/federated_learning/fed_stc.md:34
msgid "The latest global weights can be expressed as:"
msgstr "最新的 global weights 可以表示为："

#: ../../developer/algorithm/federated_learning/fed_stc.md:36
msgid "![math4](resources/fedstc_math_4.jpg)"
msgstr ""

#: ../../developer/algorithm/federated_learning/fed_stc.md:36
msgid "math4"
msgstr "math4"

#: ../../developer/algorithm/federated_learning/fed_stc.md:38
msgid ""
"When a client joins training again, it must update the corresponding "
"$P^{(s)}$ or $W$;"
msgstr "一个client再次加入训练的时候，必须更新相应的 $P^{(s)}$ 或 $W$；"

#: ../../developer/algorithm/federated_learning/fed_stc.md:40
msgid "Binarization (quant -> ternary tensor)"
msgstr "Binarization (quant -> ternary tensor)"

#: ../../developer/algorithm/federated_learning/fed_stc.md:42
msgid "$$ e' \\in {-\\mu,0,\\mu}, \\mu = mean(abs(e)) $$"
msgstr ""

#: ../../developer/algorithm/federated_learning/fed_stc.md:46
msgid ""
"Assuming that mu is the sum of the absolute values of all elements in the"
" matrix after sparse, the non-zero elements in the matrix are binarized "
"to $\\mu$ or $-\\mu$ according to the sign;"
msgstr "假设mu是sparse后的matrix中所有元素绝对值之和，matrix中非0的元素都被按照符号二值化为 $\\mu$ 或 $-\\mu$；"

#: ../../developer/algorithm/federated_learning/fed_stc.md:48
msgid "Pseudo Code on Compression"
msgstr "Pseudo Code on Compression"

#: ../../developer/algorithm/federated_learning/fed_stc.md:50
msgid "![algo](resources/fedstc_algo_1.jpg)"
msgstr "![algo](resources/fedstc_algo_1.jpg)"

#: ../../developer/algorithm/federated_learning/fed_stc.md:50
msgid "algo"
msgstr "algo"

#: ../../developer/algorithm/federated_learning/fed_stc.md:52
msgid "Lossless Encoding"
msgstr "Lossless Encoding"

#: ../../developer/algorithm/federated_learning/fed_stc.md:54
msgid "Golomb Encoding"
msgstr "Golomb Encoding"

#: ../../developer/algorithm/federated_learning/fed_stc.md:56
msgid "Experiment"
msgstr "Experiment"

#: ../../developer/algorithm/federated_learning/fed_stc.md:58
msgid "Experiment on different models + datasets:"
msgstr "在不同的模型+数据集上做实验："

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "model"
msgstr "model"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "dataset"
msgstr "dataset"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "VGG11"
msgstr "VGG11"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "CIFAR"
msgstr "CIFAR"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "CNN"
msgstr "CNN"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "KWS"
msgstr "KWS"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "LSTM"
msgstr "LSTM"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Fashion-MNIST"
msgstr "Fashion-MNIST"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "Logistic R"
msgstr "Logistic R\""

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid "MNIST"
msgstr "MNIST"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid ""
"FedAvg is one of the baselines. In order to compare the transmission cost"
" horizontally with FedSTC, FedAvg uses a delay period. For example, for "
"FedSTC with sparse rate = 1/400, the delay period is 400 iterations;"
msgstr ""
"FedAvg作为baseline之一，为了和FedSTC在传输成本上横向对比，FedAvg使用delay period，例如对sparse "
"rate = 1/400的FedSTC，delay period为400 iterations；"

#: ../../developer/algorithm/federated_learning/fed_stc.md
msgid ""
"**Experimental conclusion: FedSTC is obviously better than FedAvg in the "
"case of (a) non-iid, (b) small batch size, (c) large number of "
"participating clients but low participation in each round**"
msgstr ""
"**实验结论：FedSTC在（a）non-iid的情况下，（b）small batch "
"size的情况下，（c）参与的client数量大但每轮参与度低的情况下明显比FedAvg好**"

#: ../../developer/algorithm/federated_learning/fed_stc.md:68
msgid "on Non-iidness"
msgstr "on Non-iidness"

#: ../../developer/algorithm/federated_learning/fed_stc.md:70
msgid "outperforms FedAvg"
msgstr "outperforms FedAvg"

#: ../../developer/algorithm/federated_learning/fed_stc.md:72
msgid "![exp_1](resources/fedstc_exp_1.jpg)"
msgstr "![exp_1](resources/fedstc_exp_1.jpg)"

#: ../../developer/algorithm/federated_learning/fed_stc.md:72
msgid "exp_1"
msgstr "exp_1"

#: ../../developer/algorithm/federated_learning/fed_stc.md:74
msgid "on batch size"
msgstr "on batch size"

#: ../../developer/algorithm/federated_learning/fed_stc.md:76
msgid "![exp_2](resources/fedstc_exp_2.jpg)"
msgstr "![exp_2](resources/fedstc_exp_2.jpg)"

#: ../../developer/algorithm/federated_learning/fed_stc.md:76
msgid "exp_2"
msgstr "exp_2"

#: ../../developer/algorithm/federated_learning/fed_stc.md:78
msgid "on drop rate"
msgstr "on drop rate"

#: ../../developer/algorithm/federated_learning/fed_stc.md:80
msgid "![exp_3](resources/fedstc_exp_3.jpg)"
msgstr "![exp_3](resources/fedstc_exp_3.jpg)"

#: ../../developer/algorithm/federated_learning/fed_stc.md:80
msgid "exp_3"
msgstr "exp_3"

#: ../../developer/algorithm/federated_learning/fed_stc.md:82
msgid "on data amount unbalanced"
msgstr "on data amount unbalanced"

#: ../../developer/algorithm/federated_learning/fed_stc.md:84
msgid "![exp_4](resources/fedstc_exp_4.jpg)"
msgstr "![exp_4](resources/fedstc_exp_4.jpg)"

#: ../../developer/algorithm/federated_learning/fed_stc.md:84
msgid "exp_4"
msgstr "exp_4"

#: ../../developer/algorithm/federated_learning/fed_stc.md:86
msgid "on convergence"
msgstr "on convergence"

#: ../../developer/algorithm/federated_learning/fed_stc.md:88
msgid "![exp_5](resources/fedstc_exp_5.jpg)"
msgstr "![exp_5](resources/fedstc_exp_5.jpg)"

#: ../../developer/algorithm/federated_learning/fed_stc.md:88
msgid "exp_5"
msgstr "exp_5"

#: ../../developer/algorithm/federated_learning/fed_stc.md:90
msgid "Implementation"
msgstr "实现情况"

#: ../../developer/algorithm/federated_learning/fed_stc.md:92
msgid "The sparse+binarization in upstream and downstream has been implemented;"
msgstr "upstream和downstream中的sparse+binarization已经实现；"

#: ../../developer/algorithm/federated_learning/fed_stc.md:93
msgid "Caching is not implemented;"
msgstr "caching没有实现;"

#: ../../developer/algorithm/federated_learning/fed_stc.md:94
msgid "golomb/ encoding is not implemented;"
msgstr "golomb/ encoding没有实现；"

#: ../../developer/algorithm/federated_learning/fed_stc.md:96
msgid "Reference"
msgstr "参考文献"

#: ../../developer/algorithm/federated_learning/fed_stc.md:98
msgid ""
"[Robust and Communication-Efficient Federated Learning From Non-i.i.d. "
"Data](https://ieeexplore.ieee.org/document/8889996)"
msgstr ""

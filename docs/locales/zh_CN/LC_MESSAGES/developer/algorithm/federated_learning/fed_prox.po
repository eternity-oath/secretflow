# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Ant Group Co., Ltd.
# This file is distributed under the same license as the SecretFlow package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: SecretFlow \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-07-03 15:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../developer/algorithm/federated_learning/fed_prox.md:1
msgid "Strategy: FedProx"
msgstr "联邦策略：FedProx"

#: ../../developer/algorithm/federated_learning/fed_prox.md:3
msgid "FedAvg v.s FedProx"
msgstr "FedAvg v.s FedProx"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid "Heterogeneity"
msgstr "异构性"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid "FedAvg"
msgstr "FedAvg"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid "FedProx"
msgstr "FedProx"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid "data heterogeneity"
msgstr "数据异构性"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid "FedAvg does not guarantee fit for non-iid data"
msgstr "FedAvg不能对non-iid数据保证拟合"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid ""
"FedProx can guarantee the fitting rate for non-iid data: by changing the "
"objective function of the overall optimization, based on an assumption of"
" distribution difference, a convergence proof is obtained"
msgstr "FedProx能够对non-iid数据保证拟合速率"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid "device heterogeneity"
msgstr "设备异构性"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid ""
"FedAvg does not take into account the heterogeneity of different devices,"
" such as differences in computing power; for each device, the same "
"workload is scheduled"
msgstr "FedAvg没有考虑不同设备的异质性，例如计算力上的不同；对于每个设备来说，会安排相同的workload"

#: ../../developer/algorithm/federated_learning/fed_prox.md
msgid ""
"FedProx supports local training for each device with different workloads:"
" adjust the accuracy requirements for local training per device by "
"setting a different \"γ-inexact\" parameter for each device in each round"
msgstr ""
"FedProx支持让每个设备进行不同workload的本地训练：通过在每轮为每个设备设置一个不同的“γ "
"-inexact”参数，调整对每个设备在本地训练的精度要求；"

#: ../../developer/algorithm/federated_learning/fed_prox.md:10
msgid "Fit"
msgstr "Fit"

#: ../../developer/algorithm/federated_learning/fed_prox.md:12
msgid "Add Proximal Term to the objective function"
msgstr "在目标函数中增加 Proximal Term"

#: ../../developer/algorithm/federated_learning/fed_prox.md:14
msgid ""
"$$ \\mathop{max}\\limits_{w} h_k(w;w^t)=F_k(w)+\\frac{\\mu}{2}||w-w^t||^2"
" $$"
msgstr ""

#: ../../developer/algorithm/federated_learning/fed_prox.md:18
msgid ""
"$F_k(w)$: For a set of parameters w, the loss obtained by training k "
"local data on device k"
msgstr "$F_k(w)$: 对于一组参数 w，在设备 k 上训练 k 本地数据得到的 loss"

#: ../../developer/algorithm/federated_learning/fed_prox.md:20
msgid ""
"$w^t$: The initial model parameters sent by the server to device k in the"
" t-th round of training"
msgstr "$w^t$: 在第 t 轮训练中，服务器发送给设备 k 的初始模型参数"

#: ../../developer/algorithm/federated_learning/fed_prox.md:22
msgid "$\\mu$: a hyperparameter"
msgstr "$\\mu$: 一个超参数"

#: ../../developer/algorithm/federated_learning/fed_prox.md:24
msgid ""
"$\\mu/2||w - w^t||^2$: proximal term, which limits the difference between"
" the optimized w and the $w^t$ released in the t round, so that the "
"updated w of each device k is equal to Do not differ too much between "
"them to help fit"
msgstr ""
"$\\mu/2||w - w^t||^2$: proximal term, 限制优化后的 w 与 $w^t$ t 轮发布的差异度 "
"让每个设备k更新后的w之间不要差别过大，帮助拟合"

#: ../../developer/algorithm/federated_learning/fed_prox.md:26
msgid "Different training requirements for each device: γ-inexact"
msgstr "每个设备不同训练要求：γ -inexact"

#: ../../developer/algorithm/federated_learning/fed_prox.md:28
msgid "![definition_2](resources/fedprox_definition_2.jpg)"
msgstr "![definition_2](resources/fedprox_definition_2.jpg)"

#: ../../developer/algorithm/federated_learning/fed_prox.md:28
msgid "definition_2"
msgstr "definition_2"

#: ../../developer/algorithm/federated_learning/fed_prox.md:30
msgid ""
"FedAvg requires that each device is fully optimized for E epochs during "
"training locally; Due to equipment heterogeneity, FedProx hopes to put "
"forward different optimization requirements for each equipment k in each "
"round t, and does not require all equipment to be completely optimized;"
msgstr ""
"FedAvg 要求每个设备在本地训练时都完整地进行优化 E 个 epochs；而由于设备异质性，FedProx 希望在每一轮 t 中对每一个设备 "
"k 提出不同的优化要求，不需要所有设备都完整地进行优化；"

#: ../../developer/algorithm/federated_learning/fed_prox.md:33
msgid ""
"$\\mu_k^t \\in [0,1]$, the higher the value, the looser the constraints, "
"that is, the lower the training completion requirements for equipment k; "
"on the contrary, when $\\mu_k^t = 0$, the parameters are required the "
"training update is 0, requiring the local model to fully fit;"
msgstr ""
"$\\mu_k^t \\in [0,1]$, 值越高代表限制条件越宽松，即对设备k的训练完成度要求越低；反之，当 $\\mu_k^t = 0$ "
"的时候要求参数训练的更新为 0，要求本地模型完全拟合；"

#: ../../developer/algorithm/federated_learning/fed_prox.md:35
msgid ""
"With the help of $\\mu_k^t$, the training volume per round of each device"
" can be adjusted according to the computing resources of the device;"
msgstr "借助 $\\mu_k^t$ 可以按照设备的计算力资源调整每个设备每轮的训练量；"

#: ../../developer/algorithm/federated_learning/fed_prox.md:37
msgid "Requirements for parameter selection in Convergence analysis"
msgstr "Convergence分析中对参数选取的要求"

#: ../../developer/algorithm/federated_learning/fed_prox.md:39
msgid ""
"When the selected parameters meet the following conditions, the expected "
"value of the convergence rate of the model can be bound"
msgstr "当选取的参数满足下述条件，模型的convergence rate的期望值可以被bound"

#: ../../developer/algorithm/federated_learning/fed_prox.md:41
msgid "Parameter conditions"
msgstr "参数条件"

#: ../../developer/algorithm/federated_learning/fed_prox.md:43
msgid ""
"$$ "
"\\rho^t=(\\frac{1}{\\mu}-\\frac{\\gamma^tB}{\\mu}-\\frac{B(1+\\gamma^t\\sqrt(2))}{"
" \\overline\\mu\\sqrt(K)}-\\frac{LB(1+\\gamma^t)}{\\overline\\mu\\mu} - "
"\\frac{L(1+\\gamma^t)^2B^2} "
"{2\\mu^2}-\\frac{LB^2(1+\\gamma^t)^2}{\\mu^2K}(2\\sqrt{2K}+2)) $$"
msgstr ""

#: ../../developer/algorithm/federated_learning/fed_prox.md:47
msgid ""
"There are three groups of parameters to set: K, $\\gamma$, $\\mu$; Where "
"K is the number of client devices selected in the t round, "
"$\\gamma^t=max_k(\\gamma_k^t)$, $\\gamma$, $\\mu$ is the hyperparameter "
"for setting the proxy term. **B is a value used to assume the upper limit"
" of the current participation data distribution difference, not sure how "
"to get it**"
msgstr ""
"要设置参数有三组：K，$\\gamma$，$\\mu $；其中 K 是 t 轮中选取的 client "
"设备数量，$\\gamma^t=max_k(\\gamma_k^t)$，$\\gamma$，$\\mu$ 是设置 proximal term "
"的超参数。**B 是用来假设当前参与数据分布差异的上限值，不确定怎么得到**"

#: ../../developer/algorithm/federated_learning/fed_prox.md:50
msgid "Fitting rate"
msgstr "拟合速率"

#: ../../developer/algorithm/federated_learning/fed_prox.md:52
msgid ""
"Convergence can be proven if the parameters are set to meet the above "
"requirements"
msgstr "如果参数的设置满足上述要求，则能够证明收敛性"

#: ../../developer/algorithm/federated_learning/fed_prox.md:54
msgid "Sufficient and non-essential conditions for parameter selection"
msgstr "参数选取的充分非必要条件"

#: ../../developer/algorithm/federated_learning/fed_prox.md:56
msgid "![remark_5](resources/fedprox_remark_5.jpg)"
msgstr ""

#: ../../developer/algorithm/federated_learning/fed_prox.md:56
msgid "remark_5"
msgstr "remark_5"

#: ../../developer/algorithm/federated_learning/fed_prox.md:58
msgid ""
"There is a tradeoff between $\\gamma$ and $B$. For example, the larger "
"$B$ is, the greater the difference in data distribution, the smaller the "
"$\\gamma$ must be, and the higher the training requirements for each "
"device;"
msgstr ""
"$\\gamma$ 和 $B$ 之间存在 tradeoff，比如 $B$ 越大说明数据分布差异性越大，则 $\\gamma$ "
"必须越小，对每个设备的训练要求越高；"

#: ../../developer/algorithm/federated_learning/fed_prox.md:60
msgid "Experiment 1: Effectiveness of proximal term and inexactness"
msgstr "实验1: proximal term和inexactness的有效性"

#: ../../developer/algorithm/federated_learning/fed_prox.md:62
msgid "![figure_1](resources/fedprox_figure_1.jpg)"
msgstr "![figure_1](resources/fedprox_figure_1.jpg)"

#: ../../developer/algorithm/federated_learning/fed_prox.md:62
msgid "figure_1"
msgstr "figure_1"

#: ../../developer/algorithm/federated_learning/fed_prox.md:64
msgid "Summarize"
msgstr "总结"

#: ../../developer/algorithm/federated_learning/fed_prox.md:66
msgid ""
"The existence of $\\gamma$ and $B$ is more theoretical, in practice, the "
"workload of the device can be determined directly according to the device"
" resources"
msgstr "$\\gamma$ 和 $B$ 的定义和存在还比较理论, 在实践中可以直接按照设备资源确定设备的 workload"

#: ../../developer/algorithm/federated_learning/fed_prox.md:68
msgid "Implementation"
msgstr "实现情况"

#: ../../developer/algorithm/federated_learning/fed_prox.md:70
msgid "The proxy term for data non-iid has been implemented;"
msgstr "针对数据non-iid的proximal term已实现"

#: ../../developer/algorithm/federated_learning/fed_prox.md:71
msgid "The inexactness for device heterogeneity needs to be realized;"
msgstr "针对设备异质性的inexactness待实现"

#: ../../developer/algorithm/federated_learning/fed_prox.md:73
msgid "Reference"
msgstr "参考文献"

#: ../../developer/algorithm/federated_learning/fed_prox.md:75
msgid ""
"[Federated Optimization in Heterogeneous "
"Networks](https://arxiv.org/pdf/1812.06127.pdf)"
msgstr ""

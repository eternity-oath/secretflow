# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Ant Group Co., Ltd.
# This file is distributed under the same license as the SecretFlow package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: SecretFlow \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-09-13 20:54+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.10.3\n"

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:2
msgid "Differential Privacy: Global DP in Federated Learning"
msgstr "差分隐私：联邦学习中的Global DP"

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:4
msgid ""
"In federated learning, a trusted curator aggregates parameters optimized "
"in decentralized fashion by multiple clients. The resulting model is then"
" distributed back to all clients, ultimately converging to a joint "
"representative model. This protocol is vulnerable to differential "
"attacks, which could originate from any party during federated "
"optimization. The protocol is vulnerable to differential attacks, which "
"can come from either party during joint optimization. Analyzing the "
"distributed model can reveal the customer’s contribution during training "
"and information about their dataset. Global Differential Privacy is a "
"algorithm for client sided differential privacy preserving federated "
"optimization. The aim is to hide clients’ contributions during training, "
"balancing the trade-off between privacy loss and model performance."
msgstr ""
"在联邦学习中，一个受信任的管理者以非中心方式聚合客户端的参数。最终收敛的模型将分发给所有客户端，在整个训练过程中各参与方数据无需共享。然而，这种方案容易受到差分攻击，并且可以从参与联邦优化的任意一方中进行。这种攻击可以揭露单个客户端在训练过程中的贡献甚至泄露其数据的相关信息。Global"
" DP 是一个在联邦优化中对客户端进行差分隐私保护的算法。目的是隐藏单个客户端在训练过程中的贡献，并在隐私损失和模型精度中做平衡。"

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:18
msgid "Preliminaries"
msgstr "预备知识"

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:20
msgid ""
"We use the same definition for dp in randomized mechanisms as [1]: A "
"randomized mechanism :math:`M: D \\rightarrow R` , with domain :math:`D` "
"and range :math:`R` satisfies :math:`(\\epsilon, \\delta)`-differential "
"privacy, if for any two adjacent inputs :math:`d, d^{\\prime} \\in D` and"
" for any subset of outputs :math:`S \\subseteq R` it holds that "
":math:`P[M(d) \\in S] \\leq e^{\\epsilon} "
"\\operatorname{Pr}\\left[M\\left(d^{\\prime}\\right) \\in "
"S\\right]+\\delta` . In this definition, :math:`\\delta` accounts for the"
" probability that plain :math:`\\epsilon` -differential privacy is "
"broken."
msgstr ""
"我们对差分隐私随机化机制的定义与文献[1]相同: 一个随机机制 :math:`M: D \\rightarrow R` , 它的可行域是 "
":math:`D`，输出范围为 :math:`R`。如果输入任意两个相邻数据集 :math:`d, d^{\\prime} \\in "
"D`，其任意的输出子集 :math:`S \\subseteq R` 满足 :math:`P[M(d) \\in S] \\leq "
"e^{\\epsilon}`，则称该随机机制满足 :math:`(\\epsilon, \\delta)`-differential在该定义中， "
":math:`\\delta` 为不满足 :math:`\\epsilon` 差分隐私的概率。"

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:29
msgid ""
"The Gaussian mechanism (GM) approximates a real valued function :math:`f`"
" : :math:`D \\rightarrow R` with a differentially private mechanism. "
"Specifically, a GM adds Gaussian noise calibrated to the functions data "
"set sensitivity :math:`S_{f}` . This sensitivity is defined as the "
"maximum of the absolute distance "
":math:`\\left\\|f(d)-f\\left(d^{\\prime}\\right)\\right\\|_{2}` , where "
":math:`d^{\\prime}` and :math:`d` are two adjacent inputs. A GM is then "
"defined as :math:`M(d)=f(d)+\\mathcal{N}\\left(0, \\sigma^{2} "
"S_{f}^{2}\\right)` where :math:`\\sigma` is noise multiplier."
msgstr ""
"高斯机制（GM）对一个具有差分隐私机制的实值函数进行扰动 :math:`f` : :math:`D \\rightarrow "
"R`。高斯机制添加校准的高斯噪声到函数数据集的敏感度 :math:`S_{f}` 中。该敏感度被定义为两个相邻数据集 "
":math:`d^{\\prime}` 和 :math:`d` 的绝对距离 "
":math:`\\left\\|f(d)-f\\left(d^{\\prime}\\right)\\right\\|_{2}`。高斯机制被定义为 "
":math:`M(d)=f(d)+\\mathcal{N}\\left(0, \\sigma^{2} S_{f}^{2}\\right)`, 其中"
" :math:`\\sigma` 为噪声乘子。"

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:38
msgid ""
"It is well known that :math:`\\sigma` or :math:`\\epsilon` can be fixed "
"and evaluate an inquiry to the GM about a single approximation of "
":math:`f(d)`. For example, we assume that :math:`\\epsilon` is fixed. "
"Then, we can then bound the probability that :math:`\\epsilon` -dp is "
"broken according to: :math:`\\delta \\leq \\frac{4}{5} \\exp "
"\\left(-(\\sigma \\epsilon)^{2} / 2 \\right)` (Theorem 3.22 in [2]). It "
"should be noted that :math:`\\delta` is accumulative and grows if the "
"consecutive inquiries to the GM. Therefore, to protect privacy, an "
"accountant keeps track of :math:`\\delta` . Once a certain threshold for "
":math:`\\delta` is reached, the GM shall not answer any new inquires."
msgstr ""
"我们知道 :math:`\\epsilon` 或者 :math:`\\sigma` 可以被固定，并且使用一个查询操作来对高斯机制关于 "
":math:`f(d)` 的一个单一的近似进行评估。我们假设 :math:`\\epsilon` 被固定。然后，我们可以根据 :math:`\\delta \\leq \\frac{4}{5} \\exp \\left(-(\\sigma \\epsilon)^{2} /2 \\right)` 给出 :math:`\\epsilon` -DP被攻击的概率上界(文献[2]中的定理3.22)。应该注意到 "
":math:`\\delta` 随着对高斯机制的连续查询次数的增多而增长。因此，为了保护隐私，需要持续对 :math:`\\delta` "
"保持追踪。一旦达到关于 :math:`\\delta` 的一个确定的阈值，高斯机制将不能再回答任何新的查询。"

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:47
msgid ""
"Recently, [1] proposed a differentially private stochastic gradient "
"descent algorithm (dp-SGD). dpSGD works similar to mini-batch gradient "
"descent but the gradient averaging step is approximated by a GM. In "
"addition, the mini-batches are allocated through random sampling of the "
"data. For :math:`\\epsilon` being fixed, a privacy accountant keeps track"
" of :math:`\\delta` and stops training once a threshold is reached. "
"Intuitively, this means training is stopped once the probability that the"
" learned model reveals whether a certain data point is part of the "
"training set exceeds a certain threshold."
msgstr ""
"最近，文献[1]提出了一种差分隐私随机梯度下降算法（dp-SGD）, dp-"
"SGD与最小批次梯度下降算法类似，但是梯度平均步长通过高斯机制来扰动。此外，批量是通过数据的随机抽样分配的。因为如果 "
":math:`\\epsilon` 被固定，隐私记录可以对 :math:`\\delta` "
"进行持续追踪，一旦达到某个阈值时停止训练。换言之，一旦学习模型泄露某个数据是否属于训练集的概率超过了某一阈值，训练将会被停止。"

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:58
msgid "Method"
msgstr "方法"

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:60
msgid ""
"Global DP in Federated Learning incorporate a randomized mechanism into "
"federated learning [4]. However, opposed to [1] we do not aim at "
"protecting a single data point’s contribution in learning a model. "
"Instead, we aim at protecting a whole client’s data set. That is, we want"
" to ensure that a learned model does not reveal whether a client "
"participated during decentralized training while maintaining high model "
"performance."
msgstr ""
"联邦学习中的Global DP将随机化机制加入其中[4]。然而，与文献[1]不同，Global "
"DP的目的不是在模型学习时保护单一数据的贡献。它的目的是保护客户端的整个数据集。即，确保在非中心化训练中，学习模型将不会泄露一个客户端是否参与了训练，同时保持模型较优的精度表现。"

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:68
msgid ""
"In the framework of federated optimization [5], the central curator "
"averages client models (i.e. weight matrices) after each communication "
"round. Global DP will alter and approximate this averaging with a "
"randomized mechanism. This is done to hide a single client’s contribution"
" within the aggregation and thus within the entire decentralized learning"
" procedure. The randomized mechanism we use to approximate the average "
"consists of two steps:"
msgstr ""
"在联邦优化[5]框架中，在每一轮通信后，中心管理者对客户端模型进行了平均（例如对权重矩阵进行平均）。Global "
"DP将使用一个随机化机制改变和扰动该平均。这样做的目的是在聚合中隐藏单个客户端的贡献，从而在整个非中心化的学习过程中隐藏。用于扰动平均的随机化机制由两个步骤组成："

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:75
msgid ""
"Random sub-sampling: Let :math:`K` be the total number of clients. In "
"each communication round a random subset :math:`Z_{t}` of size "
":math:`m_{t} \\leq K` is sampled. The curator then distributes the "
"central model :math:`w_{t}` to only these clients. The central model is "
"optimized by the clients’ on their data. The clients in :math:`Z_{t}` now"
" hold distinct local models "
":math:`\\left\\{w^{k}\\right\\}_{k=0}^{m_{t}}` . The difference between "
"the optimized local model and the central model will be referred to as "
"client :math:`k` ’s update :math:`\\Delta w^{k}=w^{k}-w_{t}` . The "
"updates are sent back to the central curator at the end of each "
"communication round."
msgstr ""
"随机子采样(Random sub-sampling)：设 :math:`K` 是客户端总数。在每一轮通信中，随机采样 :math:`m_{t} "
"\\leq K` 个客户端组成随机子集 :math:`Z_{t}`。然后，管理者将中心模型 :math:`w_{t}` 同步给 "
":math:`Z_{t}` 中的客户端。各个客户端基于它们自身的数据对中心模型进行优化，则 :math:`Z_{t}` "
"中的客户端拥有了互不相同的本地模型 "
":math:`\\left\\{w^{k}\\right\\}_{k=0}^{m_{t}}`。优化后的本地模型和中心模型之间的差异被称为客户端 "
":math:`k` 的更新 :math:`\\Delta w^{k}=w^{k}-w_{t}`。然后，该更新值被发送给中心管理者。"

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:85
msgid ""
"Distorting: A Gaussian mechanism is used to distort the sum of all "
"updates. This requires knowledge about the set’s sensitivity with respect"
" to the summing operation. We can enforce a certain sensitivity by using "
"scaled versions instead of the true updates: :math:`\\triangle "
"\\bar{w}^{k}= \\triangle w^{k} / \\max \\left(1, \\frac{\\left\\|\\Delta "
"w^{k}\\right\\|_{2}}{S}\\right)` . Scaling ensures that the second norm "
"is limited :math:`\\forall k,\\left\\|\\triangle "
"\\bar{w}^{k}\\right\\|_{2}<S` . The sensitivity of the scaled updates "
"with respect to the summing operation is thus upper bounded by :math:`S` "
". The GM now adds noise (scaled to sensitivity :math:`S` ) to the sum of "
"all scaled updates. Dividing the GM’s output by :math:`m_{t}` yields an "
"approximation to the true average of all client’s updates, while "
"preventing leakage of crucial information about an individual. A new "
"central model :math:`w_{t+1}` is allocated by adding this approximation "
"to the current central model :math:`w_{t}`, and "
":math:`w_{t+1}=w_{t}+\\frac{1}{m_{t}}(\\overbrace{\\sum_{k=0}^{m_{t}} "
"\\triangle w^{k} / \\max \\left(1, \\frac{\\left\\|\\triangle "
"w^{k}\\right\\|_{2}}{S}\\right)}^{\\text {Sum of updates clipped at } "
"S}+\\overbrace{\\mathcal{N}\\left(0, \\sigma^{2} S^{2}\\right)}^{\\text "
"{Noise scaled to } S})`."
msgstr ""
"扰动(Distorting)：在这里我们使用高斯机制来扰动有更新的和。这需要我们了解集合相对于求和运算的敏感度。我们通过使用真实更新值的缩放来计算敏感度：:math:`\\triangle"
" \\bar{w}^{k}= \\triangle w^{k} / \\max \\left(1, \\frac{\\left\\|\\Delta"
" w^{k}\\right\\|_{2}}{S}\\right)`。因此，缩放后梯度相对于求和操作的灵敏度上限为 :math:`S` "
"。高斯机制将噪声添加到所有缩放后的更新值之和中。将高斯机制的输出除以 :math:`m_{t}` "
"可以得到所有客户端更新的真实平均值的扰动值。将此扰动后的更新值添加到当前中心模型可以避免关于单个客户端的重要信息的泄露。 "
":math:`w_{t}` 中，可以得到新的中心模型 "
":math:`w_{t+1}=w_{t}+\\frac{1}{m_{t}}(\\overbrace{\\sum_{k=0}^{m_{t}} "
"\\triangle w^{k} / \\max \\left(1, \\frac{\\left\\|\\triangle "
"w^{k}\\right\\|_{2}}{S}\\right)}^{\\text {Sum of updates clipped at } "
"S}+\\overbrace{\\mathcal{N}\\left(0, \\sigma^{2} S^{2}\\right)}^{\\text "
"{Noise scaled to } S})`。"

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:102
msgid ""
"In order to keep track of this privacy loss, we make use of the moments "
"accountant as proposed by Abadi et al. [3]. This accounting method "
"provides much tighter bounds on the incurred privacy loss than the "
"standard composition theorem (3.14 in [2]). Each time the curator "
"allocates a new model, the accountant evaluates :math:`\\sigma` given "
":math:`\\epsilon`, :math:`\\delta` and :math:`m` . Training shall be "
"stopped once :math:`\\epsilon` reaches a certain threshold. The choice of"
" a threshold for :math:`\\delta` depends on the total amount of clients "
":math:`K` . To ascertain that privacy for many is not preserved at the "
"expense of revealing total information about a few, we have to ensure "
"that :math:`\\delta \\ll \\frac{1}{K}` , refer to [2] chapter 2.3 for "
"more details."
msgstr ""
"为了追踪隐私损失，我们使用了由Abadi等人[3]提出的方法。这种计算方法提供了关于已有隐私损失更紧的界，相比于标准的组合定理（文献[2]中的3.14）。管理者每次同步新模型时，将会基于给定的"
" :math:`\\sigma` 去评估 :math:`\\epsilon`。一旦 :math:`\\epsilon` "
"达到某个确定的阈值，训练将停止。为了确保许多人的隐私保护不是以泄露有关少数几个信息为代价的，我们必须确保 :math:`\\delta \\ll "
"\\frac{1}{K}` ，更多细节见文献[2]的2.3节。"

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:116
msgid "Ref"
msgstr ""

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:118
msgid ""
"[1] M. Abadi, A. Chu, I. Goodfellow, H. Brendan McMahan, I. Mironov, K. "
"Talwar, and L. Zhang. Deep Learning with Differential Privacy. ArXiv "
"e-prints, 2016."
msgstr ""

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:122
msgid ""
"[2] C. Dwork and A. Roth. The algorithmic foundations of differential "
"privacy. Found. Trends Theor. Comput. Sci., 9(3–4):211–407, Aug. 2014."
msgstr ""

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:125
msgid ""
"[3] Mironov I. Rényi differential privacy[C]//2017 IEEE 30th computer "
"security foundations symposium (CSF). IEEE, 2017: 263-275."
msgstr ""

#: ../../developer/algorithm/differential_privacy/GlobalDP.rst:128
msgid ""
"[4] Geyer R C, Klein T, Nabi M. Differentially private federated "
"learning: A client level perspective[J]. arXiv preprint arXiv:1712.07557,"
" 2017."
msgstr ""

